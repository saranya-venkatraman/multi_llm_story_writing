# CollabStory: Multi-Author Story Generation

This codebase is part of the research presented in the paper: **[CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis](https://arxiv.org/abs/2406.12665)**  

# ðŸ“š Link to CollabStory Dataset

This dataset generated using this code can be found here: **[Dataset Link: CollabStory](https://huggingface.co/datasets/saranya132/CollabStory)**

## ðŸ—‚ï¸ Dataset Overview

CollabStory is a dataset of **machine-generated creative stories generated collaboratively** using up to 5 different Large Language Models (LLMs): Gemma, Olmo, Orca, Llama and Mistral. The dataset includes stories generated using single or multiple LLMs, alongside human-written stories from the WritingPrompts dataset. Details about data collection and prompts for each part can be found in the original paper (linked above).

This project generates a collaborative story using multiple language models (LLMs) in sequence. Each LLM generates a part of the story, building upon the previous parts.

## Table of Contents

- [Introduction](#introduction)
- [Features](#features)
- [Installation](#installation)
- [Usage](#usage)
- [Citation](#citation)

## Introduction

This project aims to create collaborative stories by leveraging various LLMs in a sequential manner. Each LLM contributes to the story by adding a new part, based on the previous parts generated by other models. The story-generation process includes summarization and continuation prompts to ensure coherence and creativity.

## Features

- Multi-author story generation using different LLMs
- Dynamic prompt generation for story continuation
- Summarization of the story so far for better context
- Batch processing of data for efficiency
- Customizable parameters for different use cases

## Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/.git  ## Not linked to github yet for anonymity
   ```

2. Navigate to the project directory:
   ```bash
   cd repo-folder
   ```

3. Install the required dependencies:
   ```bash
   pip install -r requirements.txt
   ```

4. Set your HuggingFace token as an environment variable:
   ```bash
   export HF_TOKEN="ENTER YOUR HUGGINGFACE TOKEN HERE"
   ```

## Usage

To run the story generation script, use the following example command:

```bash
python write_middle_parts.py --author_num 1 --start 0 --end 100 --n 10 --total_authors 5 --llm llama
```

- `author_num`: The order of this LLM or part of the story being written.
- `start`: Prompt start index (default is 0).
- `end`: Prompt end index (default is -1, which means till the end).
- `n`: Batch size.
- `total_authors`: Total number of authors contributing to the story.
- `llm`: The LLM being used to generate the story (e.g., `llama`, `mistral`,`gemma`, `olmo`, `orca`).


## Citation

If you use this dataset, please cite our paper:

> Venkatraman, S., Tripto, N. I., & Lee, D. (2024). *CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis*. arXiv preprint [arXiv:2406.12665](https://arxiv.org/abs/2406.12665).


